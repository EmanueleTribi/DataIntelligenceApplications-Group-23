{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dbefee671e6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_process\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGaussianProcessRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaussian_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRBF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConstantKernel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mGeneralLearner\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "\n",
    "class GeneralLearner:\n",
    "\n",
    "    def __init__(self, n_arms):\n",
    "        self.n_arms = n_arms\n",
    "        self.t = 0\n",
    "        self.reward_per_arm = x = [[] for i in range(n_arms)]\n",
    "        self.collected_rewards = np.array([])\n",
    "\n",
    "    def update_observations(self, pulled_arm, reward):\n",
    "        print(pulled_arm)\n",
    "        for arm in pulled_arm:\n",
    "            self.reward_per_arm[arm].append(reward)\n",
    "        self.collected_rewards = np.append(self.collected_rewards, reward)\n",
    "\n",
    "    def pull_arm(self):\n",
    "        pass    \n",
    "\n",
    "    def update(self,pulled_arm, reward):\n",
    "        self.update_observations(pulled_arm, reward)\n",
    "        \n",
    "\n",
    "class GPTS_Learner(GeneralLearner):\n",
    "    def __init__(self, n_arms, arms, adv_id):\n",
    "        super().__init__(n_arms)\n",
    "        self.arms = arms\n",
    "        self.means = np.zeros((5,self.n_arms))\n",
    "        self.sigmas = np.ones((5,self.n_arms)) * 10\n",
    "        self.pulled_arms = []\n",
    "        ## parameters of the GPTS\n",
    "        alpha = 10.0\n",
    "        kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n",
    "        self.gp = GaussianProcessRegressor(kernel=kernel, alpha=alpha ** 2, normalize_y=True, n_restarts_optimizer=9, )\n",
    "\n",
    "    ## extend the funtion update_observations of the superclass because we want to\n",
    "    ## update also the list of the pulled arms (together with the rewards and the reward per arm)\n",
    "    def update_observations(self, arm_idx, reward):\n",
    "        super().update_observations(arm_idx, reward)\n",
    "        ## per ogni categoria append l'arm corrispondente alla bid, \n",
    "        ## pulled_arms[[arm_1, arm_3, ..., ..., ...][arm2, ..., ..., ..., ...]...]\n",
    "        self.pulled_arms.append(self.arms[arm_idx])\n",
    "\n",
    "    ## funtion that updates the model(means, sigmas) looking at the new rewards obtained from the enviroment\n",
    "    def update_model(self):\n",
    "        x = np.atleast_2d(self.pulled_arms).T\n",
    "        y = self.collected_rewards\n",
    "        if len(self.pulled_arms) > 1:\n",
    "            self.gp.fit(x, y)\n",
    "        self.means, self.sigmas = self.gp.predict(np.atleast_2d(self.arms).T, return_std=True)\n",
    "        self.sigmas = np.maximum(self.sigmas, 1e-2)\n",
    "\n",
    "    ## update the value of the current round and update observation and model\n",
    "    def update(self, pulled_arm, reward):\n",
    "        self.t += 1\n",
    "        self.update_observations(pulled_arm, reward)\n",
    "        self.update_model()\n",
    "\n",
    "    ## funtion that pulls the arm, it returns the argmax of the distribution given the means and the sigmas\n",
    "    def pull_arm(self):\n",
    "        sampled_values = np.random.normal(self.means, self.sigmas)\n",
    "        return np.argmax(sampled_values,-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_arms=5\n",
    "min_bid=0.0\n",
    "max_bid=4.0\n",
    "bids=np.linspace(min_bid, max_bid, n_arms)\n",
    "sigma=10\n",
    "learner=GPTS_Learner(n_arms=n_arms, arms=bids, adv_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clicks(x):\n",
    "    return 1-np.exp(-4*x)\n",
    "\n",
    "class Advertising_envirorment:\n",
    "    def __init__(self, bids, sigma):\n",
    "        self.bids=bids\n",
    "        self.means=clicks(bids)\n",
    "        self.sigmas=np.ones((5,len(bids)))*sigma\n",
    "\n",
    "        #per ora ha una struttura semplice\n",
    "        #andrà modificato affinchè ritorni un reward in funzione delle features\n",
    "    def round(self, pulled_arm):\n",
    "        return np.sum(np.random.normal(self.means[pulled_arm], self.sigmas[pulled_arm]))\n",
    "\n",
    "    def round_all(self, pulled_arms):\n",
    "        table=[]\n",
    "        for pulled_arm in pulled_arms:\n",
    "            table.append(self.round(pulled_arm))\n",
    "        return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=12\n",
    "env= Advertising_envirorment(bids=bids, sigma=sigma)\n",
    "gpts_reward_per_experiment=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pulled arm: [2 3 4 0 0]\nreward 53.35390998654669\n[2 3 4 0 0]\npulled arm: 4\nreward -12.781267859281687\n4\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-f3de4e422a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulled_arm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpulled_arm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"reward\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulled_arm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpulled_arm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mgpts_reward_per_experiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollected_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-dbefee671e6b>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, pulled_arm, reward)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpulled_arm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulled_arm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-dbefee671e6b>\u001b[0m in \u001b[0;36mupdate_observations\u001b[0;34m(self, arm_idx, reward)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m## update also the list of the pulled arms (together with the rewards and the reward per arm)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marm_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marm_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;31m## per ogni categoria append l'arm corrispondente alla bid,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m## pulled_arms[[arm_1, arm_3, ..., ..., ...][arm2, ..., ..., ..., ...]...]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-103-dbefee671e6b>\u001b[0m in \u001b[0;36mupdate_observations\u001b[0;34m(self, pulled_arm, reward)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_observations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpulled_arm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpulled_arm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0marm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpulled_arm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_per_arm\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollected_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollected_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "for t in range(0,T-1):\n",
    "        pulled_arm=learner.pull_arm()\n",
    "        print(\"pulled arm:\",pulled_arm)\n",
    "        reward=env.round(pulled_arm=pulled_arm)\n",
    "        print(\"reward\",reward)\n",
    "        learner.update(pulled_arm=pulled_arm, reward=reward)\n",
    "    \n",
    "gpts_reward_per_experiment.append(learner.collected_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd07b65a0b764bf49cc7947e468da38261005013c03fd7f348ce5375f68cfcbfbf1",
   "display_name": "Python 3.8.8 64-bit ('projects_env': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "7b65a0b764bf49cc7947e468da38261005013c03fd7f348ce5375f68cfcbfbf1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}